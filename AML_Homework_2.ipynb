{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML_Homework_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2: Trees and Calibration\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Please push the .ipynb, .py, and .pdf to Github Classroom prior to the deadline. Please include your UNI as well.\n",
        "\n",
        "**Make sure to use the dataset that we provide in CourseWorks/Classroom. DO NOT download it from the link provided (It may be different).**\n",
        "\n",
        "Due Date : 03/02 (2nd March), 11:59 PM EST"
      ],
      "metadata": {
        "id": "4nHSeYSEQclq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name: Please input your name\n",
        "\n",
        "## UNI: Please input your UNI"
      ],
      "metadata": {
        "id": "YnGgRAIRwMlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Dataset\n",
        "Credit ([Link](https://www.kaggle.com/gamersclub/brazilian-csgo-plataform-dataset-by-gamers-club?select=tb_lobby_stats_player.csv) | [License](https://creativecommons.org/licenses/by-nc-sa/4.0/))\n",
        "\n",
        "The goal is to predict wins based on in match performace of multiple players. Please use this dataset and this task for all parts of the assignment.\n",
        "\n",
        "### Features\n",
        "\n",
        "idLobbyGame - Categorical (The Lobby ID for the game)\n",
        "\n",
        "idPlayer - Categorical (The ID of the player)\n",
        "\n",
        "idRooom - Categorical (The ID of the room)\n",
        "\n",
        "qtKill - Numerical (Number of kills)\n",
        "\n",
        "qtAssist - Numerical (Number of Assists)\n",
        "\n",
        "qtDeath - Numerical (Number of Deaths)\n",
        "\n",
        "qtHs - Numerical (Number of kills by head shot)\n",
        "\n",
        "qtBombeDefuse - Numerical (Number of Bombs Defuses)\n",
        "\n",
        "qtBombePlant - Numerical (Number of Bomb plants)\n",
        "\n",
        "qtTk - Numerical (Number of Team kills)\n",
        "\n",
        "qtTkAssist - Numerical Number of team kills assists)\n",
        "\n",
        "qt1Kill - Numerical (Number of rounds with one kill)\n",
        "\n",
        "qt2Kill - Numerical (Number of rounds with two kill)\n",
        "\n",
        "qt3Kill - Numerical (Number of rounds with three kill)\n",
        "\n",
        "qt4Kill - Numerical (Number of rounds with four kill)\n",
        "\n",
        "qt5Kill - Numerical (Number of rounds with five kill)\n",
        "\n",
        "qtPlusKill - Numerical (Number of rounds with more than one kill)\n",
        "\n",
        "qtFirstKill - Numerical (Number of rounds with first kill)\n",
        "\n",
        "vlDamage - Numerical (Total match Damage)\n",
        "\n",
        "qtHits - Numerical (Total match hits)\n",
        "\n",
        "qtShots - Numerical (Total match shots)\n",
        "\n",
        "qtLastAlive - Numerical (Number of rounds being last alive)\n",
        "\n",
        "qtClutchWon - Numerical (Number of total clutchs wons)\n",
        "\n",
        "qtRoundsPlayed - Numerical (Number of total Rounds Played)\n",
        "\n",
        "descMapName - Categorical (Map Name - de_mirage, de_inferno, de_dust2, de_vertigo, de_overpass, de_nuke, de_train, de_ancient)\n",
        "\n",
        "vlLevel - Numerical (GC Level)\n",
        "\n",
        "qtSurvived - Numerical (Number of rounds survived)\n",
        "\n",
        "qtTrade - Numerical (Number of trade kills)\n",
        "\n",
        "qtFlashAssist - Numerical (Number of flashbang assists)\n",
        "\n",
        "qtHitHeadshot - Numerical (Number of times the player hit headshot\n",
        "\n",
        "qtHitChest - Numerical (Number of times the player hit chest)\n",
        "\n",
        "qtHitStomach - Numerical (Number of times the player hit stomach)\n",
        "\n",
        "qtHitLeftAtm - Numerical (Number of times the player hit left arm)\n",
        "\n",
        "qtHitRightArm - Numerical (Number of times the player hit right arm)\n",
        "\n",
        "qtHitLeftLeg - Numerical (Number of times the player hit left leg)\n",
        "\n",
        "qtHitRightLeg - Numerical (Number of times the player hit right leg)\n",
        "\n",
        "flWinner - Winner Flag (**Target Variable**).\n",
        "\n",
        "dtCreatedAt - Date at which this current row was added. (Date)\n"
      ],
      "metadata": {
        "id": "bbOG2REfo0bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: Decision Trees"
      ],
      "metadata": {
        "id": "gFh2_utPbAK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1: Load the provided dataset**"
      ],
      "metadata": {
        "id": "SCEJZC5bFjin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t7_jJvfqk5Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2: Plot % of missing values in each column. Would you consider dropping any columns? Assuming we want to train a decision tree, would you consider imputing the missing values? If not, why? (Remove the columns that you consider dropping - you must remove the dtCreatedAt column)**"
      ],
      "metadata": {
        "id": "C88LB8AAdr6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qsSxhcZnk5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3: Plot side-by-siide bars of class distribtuion for each category for the categorical feature and the target categories.**"
      ],
      "metadata": {
        "id": "Sfu-N0jQeL5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZCmGclwxk6d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4: Split the data into development and test datasets. Which splitting methodology did you choose and why?**"
      ],
      "metadata": {
        "id": "TzFoX1w5edPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FG0bjSZxk7K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.5: Preprocess the data (Handle the Categorical Variable). Do we need to apply scaling? Briefly Justify**"
      ],
      "metadata": {
        "id": "2grWnbK5eqmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mVQjLPO3k7nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.6: Fit a Decision Tree on the development data until all leaves are pure. What is the performance of the tree on the development set and test set? Provide metrics you believe are relevant and briefly justify.**"
      ],
      "metadata": {
        "id": "QOsjqLz8e2xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xGNUFbm-k8Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.7: Visualize the trained tree until the max_depth 8**"
      ],
      "metadata": {
        "id": "Y9vSrFM-fqdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "skZoUeXwk8sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.8: Prune the tree using one of the techniques discussed in class and evaluate the performance**"
      ],
      "metadata": {
        "id": "v_5pPhS2f12o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e9citpjBk9Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.9: List the top 3 most important features for this trained tree? How would you justify these features being the most important?**"
      ],
      "metadata": {
        "id": "7by9pkAJf-7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iMokROlNf1P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Random Forests"
      ],
      "metadata": {
        "id": "javStHTHbHXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sbKTHdeRk9sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1: Train a Random Forest model on the development dataset using RandomForestClassifier class in sklearn. Use the default parameters. Evaluate the performance of the model on test dataset. Does this perform better than Decision Tree on the test dataset (compare to results in Q 1.6)?**"
      ],
      "metadata": {
        "id": "dDl3lnx6GAT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RlN4aOX_l2Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2: Does all trees in the trained random forest model have pure leaves? How would you verify this?**"
      ],
      "metadata": {
        "id": "7oSKB0j4gi2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BQ32vtYEl3c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3: Assume you want to improve the performance of this model. Also, assume that you had to pick two hyperparameters that you could tune to improve its performance. Which hyperparameters would you choose and why?**\n"
      ],
      "metadata": {
        "id": "jKtznt_egqD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "um21KRKDl56w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4: Now, assume you had to choose up to 5 different values (each) for these two hyperparameters. How would you choose these values that could potentially give you a performance lift?**"
      ],
      "metadata": {
        "id": "XZGgPLucgy0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qV-VeSAnl7su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5: Perform model selection using the chosen values for the hyperparameters. Use cross-validation for finding the optimal hyperparameters. Report on the optimal hyperparameters. Estimate the performance of the optimal model (model trained with optimal hyperparameters) on test dataset? Has the performance improved over your plain-vanilla random forest model trained in Q2.1?**"
      ],
      "metadata": {
        "id": "F4CS841Ag7YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3BfdHWqRl8uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.6: Can you find the top 3 most important features from the model trained in Q2.5? How do these features compare to the important features that you found from Q1.9? If they differ, which feature set makes more sense?**"
      ],
      "metadata": {
        "id": "GAdrafXxhBJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g3QHTi1Og60-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Gradient Boosted Trees"
      ],
      "metadata": {
        "id": "6ttEhHqqbOZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1: Choose three hyperparameters to tune GradientBoostingClassifier and HistGradientBoostingClassifier on the development dataset using 5-fold cross validation. Report on the time taken to do model selection for both the models. Also, report the performance of the test dataset from the optimal models.**"
      ],
      "metadata": {
        "id": "g4q24mXyGCjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I3CRf47poZzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2: Train an XGBoost model by tuning 3 hyperparameters using 5 fold cross-validation. Compare the performance of the trained XGBoost model on the test dataset against the performances obtained from 3.1**"
      ],
      "metadata": {
        "id": "aoOLJ1SJhRaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cihn4VkqoaMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3: Compare the results on the test dataset from XGBoost, HistGradientBoostingClassifier, GradientBoostingClassifier with results from Q1.6 and Q2.1. Which model tends to perform the best and which one does the worst? How big is the difference between the two? Which model would you choose among these 5 models and why?**"
      ],
      "metadata": {
        "id": "EvUUJPhkhneh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KFlxuwWMoa35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4: Can you list the top 3 features from the trained XGBoost model? How do they differ from the features found from Random Forest and Decision Tree? Which one would you trust the most?**"
      ],
      "metadata": {
        "id": "bLzaOv8khtiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Id887PsdobRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.5: Can you choose the top 7 features (as given by feature importances from XGBoost) and repeat Q3.2? Does this model perform better than the one trained in Q3.2? Why or why not is the performance better?**"
      ],
      "metadata": {
        "id": "u13rx2hyhz3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uMum69ypobrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: Calibration"
      ],
      "metadata": {
        "id": "NEEmj4qsbnJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1: Estimate the brier score for the XGBoost model (trained with optimal hyperparameters from Q3.2) scored on the test dataset.**"
      ],
      "metadata": {
        "id": "4avkwE8sGFhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MIVBqrqrocKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2: Calibrate the trained XGBoost model using isotonic regression as well as Platt scaling. Plot predicted v.s. actual on test datasets from both the calibration methods**"
      ],
      "metadata": {
        "id": "k8J-WzSvh-i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VZJiSfhcocsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3: Report brier scores from both the calibration methods. Do the calibration methods help in having better predicted probabilities?**"
      ],
      "metadata": {
        "id": "_BufbZfmiCjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cLwrHlRXodI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}